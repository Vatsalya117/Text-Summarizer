<section id="datasets">
    <h2>Datasets</h2>
    <p>
        In order to train our GAN-based summarization model, we used several large-scale text datasets 
        from a variety of sources. These datasets are representative of different text domains and styles, 
        which helped to generalize the model for diverse types of text summarization tasks.
    </p>
    <ul>
        <li><strong>Wikipedia Corpus:</strong> A collection of large articles from Wikipedia, providing 
        a wide range of topics and writing styles. Summarizing articles from this dataset is a significant 
        challenge due to the depth and variety of content.</li>
        <li><strong>News Articles Dataset:</strong> This dataset contains news reports from various publishers, 
        covering a broad range of subjects such as politics, science, and entertainment. News articles tend to 
        have a structured format, which offers a different type of summarization challenge.</li>
        <li><strong>Scientific Papers Dataset:</strong> A specialized dataset containing academic papers 
        across various scientific fields. Summarizing such documents requires a deep understanding of 
        technical terms and concepts, making this one of the more difficult datasets for abstractive summarization.</li>
        <li><strong>Social Media Data:</strong> This dataset consists of posts and comments from platforms 
        like Twitter and Reddit. The informal language used here presents unique challenges for text summarization.</li>
    </ul>
</section>
